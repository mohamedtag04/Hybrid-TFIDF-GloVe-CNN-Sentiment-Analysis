{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8240,"sourceType":"datasetVersion","datasetId":5504}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install torch transformers numpy pandas scikit-learn gensim datasets","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-10T20:37:33.044230Z","iopub.execute_input":"2025-05-10T20:37:33.044715Z","iopub.status.idle":"2025-05-10T20:39:01.174376Z","shell.execute_reply.started":"2025-05-10T20:37:33.044686Z","shell.execute_reply":"2025-05-10T20:39:01.173505Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nCollecting scipy>=1.3.2 (from scikit-learn)\n  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec (from torch)\n  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, scipy\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.15.2\n    Uninstalling scipy-1.15.2:\n      Successfully uninstalled scipy-1.15.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\nnilearn 0.11.1 requires scikit-learn>=1.4.0, but you have scikit-learn 1.2.2 which is incompatible.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.5 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fsspec-2024.12.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 scipy-1.13.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer, BertModel\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\nimport pandas as pd\nfrom datasets import load_dataset\nfrom gensim.models import KeyedVectors\nimport nltk\nimport joblib\nnltk.download('punkt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T18:49:35.891389Z","iopub.execute_input":"2025-05-10T18:49:35.891620Z","iopub.status.idle":"2025-05-10T18:51:14.313809Z","shell.execute_reply.started":"2025-05-10T18:49:35.891601Z","shell.execute_reply":"2025-05-10T18:51:14.312987Z"}},"outputs":[{"name":"stderr","text":"2025-05-10 18:49:48.817554: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746902989.084770      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746902989.157223      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T18:51:14.314683Z","iopub.execute_input":"2025-05-10T18:51:14.315392Z","iopub.status.idle":"2025-05-10T18:51:14.319291Z","shell.execute_reply.started":"2025-05-10T18:51:14.315373Z","shell.execute_reply":"2025-05-10T18:51:14.318480Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class IMDbDataset(Dataset):\n    def __init__(self, data, tokenizer, tfidf_vectorizer, glove_model, max_len=128):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.tfidf_vectorizer = tfidf_vectorizer\n        self.glove_model = glove_model \n        self.max_len = max_len\n        self.texts = data['text']\n        self.labels = data['label']\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        label = self.labels[idx]\n\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n\n        input_ids = encoding['input_ids'].flatten()\n        attention_mask = encoding['attention_mask'].flatten()\n\n        tfidf_scores = self.tfidf_vectorizer.transform([text]).toarray()[0]\n        words = text.lower().split()\n        tfidf_dict = {word: score for word, score in zip(self.tfidf_vectorizer.get_feature_names_out(), tfidf_scores) if score > 0}\n\n        glove_embeds = np.zeros((self.max_len, 300)) \n        tokens = self.tokenizer.convert_ids_to_tokens(input_ids)\n        for i, token in enumerate(tokens):\n            if i >= self.max_len:\n                break\n            word = token if not token.startswith('##') and token not in ['[CLS]', '[SEP]', '[PAD]'] else ''\n            if word and word in self.glove_model:\n                emb = self.glove_model[word]\n                weight = tfidf_dict.get(word, 1.0) \n                glove_embeds[i] = emb * weight\n\n        return {\n            'input_ids': input_ids.to(device),\n            'attention_mask': attention_mask.to(device),\n            'glove_embeds': torch.tensor(glove_embeds, dtype=torch.float).to(device),\n            'label': torch.tensor(label, dtype=torch.long).to(device)\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T18:51:14.321090Z","iopub.execute_input":"2025-05-10T18:51:14.321387Z","iopub.status.idle":"2025-05-10T18:51:14.343689Z","shell.execute_reply.started":"2025-05-10T18:51:14.321368Z","shell.execute_reply":"2025-05-10T18:51:14.343157Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class HybridBertCNN(nn.Module):\n    def __init__(self, bert_model, glove_dim=300, dropout=0.3):\n        super(HybridBertCNN, self).__init__()\n        self.bert = bert_model\n        bert_dim = 768 \n        self.conv3 = nn.Conv1d(in_channels=bert_dim + glove_dim, out_channels=128, kernel_size=3, padding=1)\n        self.conv4 = nn.Conv1d(in_channels=bert_dim + glove_dim, out_channels=128, kernel_size=4, padding=2)\n        self.conv5 = nn.Conv1d(in_channels=bert_dim + glove_dim, out_channels=128, kernel_size=5, padding=2)\n        self.attention = nn.MultiheadAttention(embed_dim=384, num_heads=8) \n        self.fc1 = nn.Linear(384, 128)\n        self.fc2 = nn.Linear(128, 2) \n        self.dropout = nn.Dropout(dropout)\n        self.relu = nn.ReLU()\n\n    def forward(self, input_ids, attention_mask, glove_embeds):\n        with torch.no_grad():  \n            bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        bert_embeds = bert_output.last_hidden_state  \n        combined_embeds = torch.cat((bert_embeds, glove_embeds), dim=-1) \n        combined_embeds = combined_embeds.permute(0, 2, 1)\n        conv3_out = self.relu(self.conv3(combined_embeds))\n        conv4_out = self.relu(self.conv4(combined_embeds))\n        conv5_out = self.relu(self.conv5(combined_embeds))\n        conv3_out = torch.max(conv3_out, dim=-1)[0]\n        conv4_out = torch.max(conv4_out, dim=-1)[0]\n        conv5_out = torch.max(conv5_out, dim=-1)[0]\n        cnn_out = torch.cat((conv3_out, conv4_out, conv5_out), dim=-1)  \n        cnn_out = cnn_out.unsqueeze(0)  \n        attn_out, _ = self.attention(cnn_out, cnn_out, cnn_out)\n        attn_out = attn_out.squeeze(0)\n        out = self.dropout(attn_out)\n        out = self.relu(self.fc1(out))\n        out = self.dropout(out)\n        out = self.fc2(out)\n\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T18:51:14.344409Z","iopub.execute_input":"2025-05-10T18:51:14.344630Z","iopub.status.idle":"2025-05-10T18:51:14.366277Z","shell.execute_reply.started":"2025-05-10T18:51:14.344607Z","shell.execute_reply":"2025-05-10T18:51:14.365660Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, epochs=3, lr=2e-5):\n    optimizer = optim.AdamW(model.parameters(), lr=lr)\n    loss_fn = nn.CrossEntropyLoss()\n\n    for epoch in range(epochs):\n        model.train()\n        train_loss = 0\n        for batch in train_loader:\n            optimizer.zero_grad()\n            input_ids = batch['input_ids']\n            attention_mask = batch['attention_mask']\n            glove_embeds = batch['glove_embeds']\n            labels = batch['label']\n\n            outputs = model(input_ids, attention_mask, glove_embeds)\n            loss = loss_fn(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n        model.eval()\n        val_loss = 0\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for batch in val_loader:\n                input_ids = batch['input_ids']\n                attention_mask = batch['attention_mask']\n                glove_embeds = batch['glove_embeds']\n                labels = batch['label']\n\n                outputs = model(input_ids, attention_mask, glove_embeds)\n                loss = loss_fn(outputs, labels)\n                val_loss += loss.item()\n\n                preds = torch.argmax(outputs, dim=1)\n                correct += (preds == labels).sum().item()\n                total += labels.size(0)\n\n        print(f\"Epoch {epoch+1}/{epochs}\")\n        print(f\"Train Loss: {train_loss / len(train_loader):.4f}\")\n        print(f\"Val Loss: {val_loss / len(val_loader):.4f}\")\n        print(f\"Val Accuracy: {correct / total:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T18:51:14.366974Z","iopub.execute_input":"2025-05-10T18:51:14.367199Z","iopub.status.idle":"2025-05-10T18:51:14.389487Z","shell.execute_reply.started":"2025-05-10T18:51:14.367183Z","shell.execute_reply":"2025-05-10T18:51:14.388768Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"dataset = load_dataset(\"imdb\")\ntrain_data = dataset['train']\nval_data = dataset['test']\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nbert_model = BertModel.from_pretrained('bert-base-uncased').to(device)\ntfidf_vectorizer = TfidfVectorizer(max_features=5000)\ntfidf_vectorizer.fit([str(text) for text in train_data['text']])\nglove_path = \"/kaggle/input/glove6b300dtxt/glove.6B.300d.txt\"\nglove_model = KeyedVectors.load_word2vec_format(glove_path, binary=False, no_header=True)\nprint(\"GloVe embeddings loaded successfully.\")\n\ntrain_dataset = IMDbDataset(train_data, tokenizer, tfidf_vectorizer, glove_model)\nval_dataset = IMDbDataset(val_data, tokenizer, tfidf_vectorizer, glove_model)\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n\nmodel = HybridBertCNN(bert_model).to(device)\ntrain_model(model, train_loader, val_loader, epochs = 4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T18:51:14.390096Z","iopub.execute_input":"2025-05-10T18:51:14.390347Z","iopub.status.idle":"2025-05-10T19:47:42.463920Z","shell.execute_reply.started":"2025-05-10T18:51:14.390331Z","shell.execute_reply":"2025-05-10T19:47:42.463150Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/7.81k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69a47e67d6834a59a5d5a5df06e4d6af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cee95c6cf5024f75b2ef0c88b19f729b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"293873b970174b0fa742688bf5cb6105"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4991e0c1cde4414b404dd8c8e05d49f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"384a3ff7d55e43baa3c2f1ba9f2c71a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"986d51b08199408da35bf3df8d23f3ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ba31eb797f545cb8396814d521f2dd0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99f952eff6d44ced8e08b45c8e5cc3ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"120bd5f482074da6a886ff18878492df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7286720e7a643b4949311b5285cd8f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8245ee8e24b1495f8f1820fe4a9f910a"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e84bc36cf8b64e0ea209b63b2dacc5cf"}},"metadata":{}},{"name":"stdout","text":"GloVe embeddings loaded successfully.\nEpoch 1/4\nTrain Loss: 0.4707\nVal Loss: 0.3473\nVal Accuracy: 0.8478\nEpoch 2/4\nTrain Loss: 0.3383\nVal Loss: 0.3265\nVal Accuracy: 0.8590\nEpoch 3/4\nTrain Loss: 0.3020\nVal Loss: 0.3136\nVal Accuracy: 0.8677\nEpoch 4/4\nTrain Loss: 0.2614\nVal Loss: 0.3140\nVal Accuracy: 0.8697\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"def predict_sentiment(model, tokenizer, tfidf_vectorizer, glove_model, text, max_len=128):\n    model.eval()\n    text = str(text)\n    encoding = tokenizer.encode_plus(\n        text,\n        add_special_tokens=True,\n        max_length=max_len,\n        return_token_type_ids=False,\n        padding='max_length',\n        truncation=True,\n        return_attention_mask=True,\n        return_tensors='pt'\n    )\n\n    input_ids = encoding['input_ids'].flatten().to(device)\n    attention_mask = encoding['attention_mask'].flatten().to(device)\n    tfidf_scores = tfidf_vectorizer.transform([text]).toarray()[0]\n    words = text.lower().split()\n    tfidf_dict = {word: score for word, score in zip(tfidf_vectorizer.get_feature_names_out(), tfidf_scores) if score > 0}\n\n    glove_embeds = np.zeros((max_len, 300))\n    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n    for i, token in enumerate(tokens):\n        if i >= max_len:\n            break\n        word = token if not token.startswith('##') and token not in ['[CLS]', '[SEP]', '[PAD]'] else ''\n        if word and word in glove_model:\n            emb = glove_model[word]\n            weight = tfidf_dict.get(word, 1.0) \n            glove_embeds[i] = emb * weight\n\n    glove_embeds = torch.tensor(glove_embeds, dtype=torch.float).to(device)\n    with torch.no_grad():\n        outputs = model(input_ids.unsqueeze(0), attention_mask.unsqueeze(0), glove_embeds.unsqueeze(0))\n        preds = torch.argmax(outputs, dim=1).item()\n    label = \"Positive\" if preds == 1 else \"Negative\"\n    return label, outputs.softmax(dim=1).cpu().numpy()[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T19:49:01.538648Z","iopub.execute_input":"2025-05-10T19:49:01.538926Z","iopub.status.idle":"2025-05-10T19:49:01.547868Z","shell.execute_reply.started":"2025-05-10T19:49:01.538904Z","shell.execute_reply":"2025-05-10T19:49:01.547031Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"example_text = \"the person sitting infront of me in the cinema was awful though and kept talking which ruined the experience and made it so bad so I won't be going to this cinema again, but the movie itself is very good\"\npredicted_label, probabilities = predict_sentiment(model, tokenizer, tfidf_vectorizer, glove_model, example_text)\nprint(f\"Predicted Sentiment: {predicted_label}\")\nprint(f\"Probabilities (Negative, Positive): {probabilities}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T19:49:07.160846Z","iopub.execute_input":"2025-05-10T19:49:07.161383Z","iopub.status.idle":"2025-05-10T19:49:07.193862Z","shell.execute_reply.started":"2025-05-10T19:49:07.161359Z","shell.execute_reply":"2025-05-10T19:49:07.193307Z"}},"outputs":[{"name":"stdout","text":"Predicted Sentiment: Negative\nProbabilities (Negative, Positive): [0.6360805  0.36391953]\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"# saving model weights","metadata":{}},{"cell_type":"code","source":"torch.save(model.state_dict(), 'hybrid_bert_cnn_weights.pth')\njoblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T19:49:10.411505Z","iopub.execute_input":"2025-05-10T19:49:10.412109Z","iopub.status.idle":"2025-05-10T19:49:11.273456Z","shell.execute_reply.started":"2025-05-10T19:49:10.412086Z","shell.execute_reply":"2025-05-10T19:49:11.272672Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"['tfidf_vectorizer.pkl']"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"# loading weights","metadata":{}},{"cell_type":"code","source":"model = HybridBertCNN(bert_model).to(device)\nmodel.load_state_dict(torch.load('hybrid_bert_cnn_weights.pth'))\ntfidf_vectorizer = joblib.load('tfidf_vectorizer.pkl')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T19:51:59.940790Z","iopub.execute_input":"2025-05-10T19:51:59.941152Z","iopub.status.idle":"2025-05-10T19:52:00.500262Z","shell.execute_reply.started":"2025-05-10T19:51:59.941098Z","shell.execute_reply":"2025-05-10T19:52:00.499648Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/1438076511.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('hybrid_bert_cnn_weights.pth'))\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"example_text = \"the person sitting infront of me in kept talking which ruined the experience so I won't be going to this cinema again, but the movie itself is very good\"\npredicted_label, probabilities = predict_sentiment(model, tokenizer, tfidf_vectorizer, glove_model, example_text)\nprint(f\"Predicted Sentiment: {predicted_label}\")\nprint(f\"Probabilities (Negative, Positive): {probabilities}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T19:52:26.553479Z","iopub.execute_input":"2025-05-10T19:52:26.553766Z","iopub.status.idle":"2025-05-10T19:52:26.586899Z","shell.execute_reply.started":"2025-05-10T19:52:26.553748Z","shell.execute_reply":"2025-05-10T19:52:26.586281Z"}},"outputs":[{"name":"stdout","text":"Predicted Sentiment: Positive\nProbabilities (Negative, Positive): [0.18203108 0.8179689 ]\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}